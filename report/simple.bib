@web_page{learningtorank,
   title = {Learning to Rank: A Complete Guide to Ranking using Machine Learning | by Francesco Casalegno | Towards Data Science},
   url = {https://towardsdatascience.com/learning-to-rank-a-complete-guide-to-ranking-using-machine-learning-4c9688d370d4},
}

@web_page{vectormodel,
   title = {Vector space model - Wikipedia},
   url = {https://en.wikipedia.org/wiki/Vector_space_model},
}

@article{mitra2018an,
author = {Mitra, Bhaskar and Craswell, Nick},
title = {An Introduction to Neural Information Retrieval},
year = {2018},
month = {December},
abstract = {Neural ranking models for information retrieval (IR) use shallow or deep neural networks to rank search results in response to a query. Traditional learning to rank models employ supervised machine learning (ML) techniques—including neural networks—over hand-crafted IR features. By contrast, more recently proposed neural models learn representations of language from raw text that can bridge the gap between query and document vocabulary. Unlike classical learning to rank models and non-neural approaches to IR, these new ML techniques are data-hungry, requiring large scale training data before they can be deployed. This tutorial introduces basic concepts and intuitions behind neural IR models, and places them in the context of classical non-neural approaches to IR. We begin by introducing fundamental concepts of retrieval and different neural and non-neural approaches to unsupervised learning of vector representations of text. We then review IR methods that employ these pre-trained neural vector representations without learning the IR task end-to-end. We introduce the Learning to Rank (LTR) framework next, discussing standard loss functions for ranking. We follow that with an overview of deep neural networks (DNNs), including standard architectures and implementations. Finally, we review supervised neural learning to rank models, including recent DNN architectures trained end-to-end for ranking tasks. We conclude with a discussion on potential future directions for neural IR.},
url = {https://www.microsoft.com/en-us/research/publication/introduction-neural-information-retrieval/},
pages = {1-126},
journal = {Foundations and Trends® in Information Retrieval},
volume = {13},
number = {1},
}

@article{wu2017convolutional,
	title={Convolutional recurrent neural networks for hyperspectral data classification},
	author={Wu, Hao and Prasad, Saurabh},
	journal={Remote Sensing},
	volume={9},
	number={3},
	pages={298},
	year={2017},
	publisher={MDPI}
}


